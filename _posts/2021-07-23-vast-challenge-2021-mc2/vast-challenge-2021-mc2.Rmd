---
title: "VAST Challenge 2021 MC2 (I)"
description: |
  To visualize & analyze card usage and car movement data with the employee disappearance incident
author:
  - name: LIU Yangguang
    url: https://www.linkedin.com/in/ygliu/
    affiliation: School of Computing and Information Systems, Singapore Management University
date: 07-23-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 3
    toc_float: true
    #code_folding: true
categories:
  - R
  - Visualization
  - Interactive Charts
preview: bkg.png
---


<style> /* A floating TOC, but it's not suitable for long TOC*/
html {
  scroll-behavior: auto; /* smooth, auto */
}
d-article {
    contain: none;
    overflow-x: hidden;
  }
#TOC {
  position: relative; /* float will make the toc fixed; 'fixed' can make toc float */
  z-index: auto; /* priority when the elements overlap each other,-1,auto,50  */
  background: transparent;     /* or#ebebeb; or white, or transparent */
  /* optional padding: 10px; border-radius: 5px; */
  }

/* Hide the ToC when resized to mobile or tablet:  480px, 768px, 900px */
@media screen and (min-height: 50em) and (min-width: 80em) { /* change from 900 , min-width: 80em, min-width: 1000px*/
#TOC {
    position: fixed;
  }
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE,
                      message = FALSE,
                      warning = FALSE)
```


## Background

This study is based on the [Mini-Challenge 2](https://vast-challenge.github.io/2021/MC2.html) of the [VAST Challenge 2021](https://vast-challenge.github.io/2021/). In a fiction scenario, there is a natural gas company named "GASTech" operating in the island country if Kronos. The GASTech didn't do well in environment stewardship. And after an company IPO celebration in January 2014, several employees of GASTech went missing. An environment organization is suspected in the disappearance.

Many of the Abila, Kronos-based employees of GASTech have company cars which are approved for both personal and business use. Those who do not have company cars have the ability to check out company trucks for business use, but these trucks cannot be used for personal business.

Employees with company cars are happy to have these vehicles, because the company cars are generally much higher quality than the cars they would be able to afford otherwise. However, GASTech does not trust their employees. Without the employees? knowledge, GASTech has installed geospatial tracking software in the company vehicles. The vehicles are tracked periodically as long as they are moving.

This vehicle tracking data has been made available to law enforcement to support their investigation. Unfortunately, data is not available for the day the GASTech employees went missing. Data is only available for the two weeks prior to the disappearance.

To promote local businesses, Kronos based companies provide a Kronos Kares benefit card to GASTech employees giving them discounts and rewards in exchange for collecting information about their credit card purchases and preferences as recorded on loyalty cards. This data has been made available to investigators in the hopes that it can help resolve the situation. However, Kronos Kares does not collect personal information beyond purchases.

### Requirement

Use visual analytics to identify which GASTech employees made which purchases and identify suspicious patterns of behavior. Besides, the study must cope with uncertainties that result from missing, conflicting, and imperfect data to make recommendations for further investigation.

### Questions

1. Using just the credit and loyalty card data, identify the most popular locations, and when they are popular. What anomalies do you see? What corrections would you recommend to correct these anomalies? Please limit your answer to 8 images and 300 words.

2. Add the vehicle data to your analysis of the credit and loyalty card data. How does your assessment of the anomalies in question 1 change based on this new data? What discrepancies between vehicle, credit, and loyalty card data do you find? Please limit your answer to 8 images and 500 words.

3. Can you infer the owners of each credit card and loyalty card? What is your evidence? Where are there uncertainties in your method? Where are there uncertainties in the data? Please limit your answer to 8 images and 500 words.

4. Given the data sources provided, identify potential informal or unofficial relationships among GASTech personnel. Provide evidence for these relationships. Please limit your response to 8 images and 500 words.

5. Do you see evidence of suspicious activity? Identify 1- 10 locations where you believe the suspicious activity is occurring, and why Please limit your response to 10 images and 500 words.

### Literature review

The VAST Challenge 2014 has the same scenario with slightly different dataset and questions. The submission repository can be found [here](http://visualdata.wustl.edu/varepository/VAST%20Challenge%202014/challenges/MC2%20-%20Patterns%20of%20Life%20Analysis/).

Various analytic tools were used among the submissions, like SAS EM, D3 and custom tools. From the graph view, the heatmap and time histograms were useful to represent the numerical value under the combination of two discrete/categorical variables, such as the hourly temperature of different regions in one day. Besides, car movement line/dot graph on the map can help to track suspicious activities. And the relationships among individuals are shown represented well in network graphs.

However, almost all graphs were static and readers would find it difficult to explore other parts in graphs which were not specially mentioned by authors. Since the study is displayed on html pages, interactive graphs will be possible and add more details & convenience. For example, the tooltip function can make every data point to have detailed information without checking the axis or drawing additional graphs. The zoom-in and on-click functions allow readers to check or focus on one part in a complex graph with many lines/objects.  



## Data Preparation

### Data Wrangling

Import packages.

```{r}
library(tidyverse)
library(lubridate)
library(raster)
library(sf)
library(tidyr)
library(dplyr)
```


##### Card Usage data

We have the consumption records of credit cards and loyalty cards.

```{r}
loyalty <- read_csv("data/loyalty_data.csv", locale=locale(encoding ="windows-1252"))
cc <- read_csv("data/cc_data.csv", locale=locale(encoding ="windows-1252"))
# The location names contain some special characters, such as "Café", which are not recognized by utf-8 encoding. Thus, special encoding is used in reading data.
```


Take a glimpse of credit card data and loyalty card data

```{r}
knitr::kable(cc[c(0:5),],
             caption = "Credit Card  Usage Data") %>% 
  kableExtra::kable_paper("hover", full_width = F)
```

```{r}
knitr::kable(loyalty[c(0:5),],
             caption = "Loyalty Card Usage Data") %>% 
  kableExtra::kable_paper("hover", full_width = F)
```

The timestamp in the credit card usage date ("cc") contains date and time, while the timestamp in the loyal card usage data ("loyalty") contains only data. Besides, their data type is string, which will be transformed into datetime type.

After that, we will separate day, hour from the datetime feature. It will make it simple to draw graphs later.

```{r}
loyalty$timestamp <- as.Date(loyalty$timestamp, "%m/%d/%Y")
cc$timestamp <- strptime(cc$timestamp, "%m/%d/%Y %H:%M")
# separate features
loyalty$day <- mday(loyalty$timestamp)
cc$date <- as.Date(cc$timestamp, "%m/%d/%Y %H:%M")
cc$day <- mday(cc$date)
cc$hour <- hour(cc$timestamp)
```

##### GPS and car assignments

The car movement GPS data and the car assignment data are provided.

```{r}
gps <- read_csv("data/gps.csv")

knitr::kable(gps[c(0:5),],
             caption = "GPS Data") %>% 
  kableExtra::kable_paper("hover", full_width = F)
```

The timestamp in the GPS data also need to be transformed. 

Besides, the longitude and latitude will be rounded into 5 digits. It can avoid the inconsistent/inaccurate in GPS data to some extent. And five decimal places implies 1.11 meters accuracy, which is better than 4 or 6 digits (11.1 meter or 0.11 meter accuracy) under this question scenario.


```{r}
# transform features
gps$Timestamp <- strptime(gps$Timestamp, "%m/%d/%Y %H:%M:%S")
gps$day <- mday(gps$Timestamp)
### round the gps into 5 digits
gps$lat <- round(gps$lat, digits = 5)
gps$long <- round(gps$long, digits = 5)
# use individual gps2 to find stop locations
gps2 <- gps
```

In following sections, we will draw car stop locations as well as car movement paths on the map. Thus, we need to get the car stop locations and car movement from the GPS data. 

And in the question background of the [challenge page](https://vast-challenge.github.io/2021/MC2.html), it mentioned that the vehicles are tracked periodically as long as they are moving. 

So the time gap in the time sequential GPS data within one car could indicate that this car stopped at current GPS location. To find places of interest, we will exclude out the time gap less than 3 minutes, which might be that the car stopped to wait for traffic light or just GPS tracker problem. 


```{r}
gps2 <- gps
gps2 <- gps2 %>% 
  group_by(id) %>% 
  # use the current timestamp minus the timestamp in a preceding record
  mutate(end = Timestamp,
         start = lag(Timestamp, default = first(Timestamp),
                   order_by = Timestamp),
         diff_mins = difftime(end, start, units = "mins")) %>% 
  # diff_mins shows the time gap between the current record and the preceding record
  # if the diff_mins > 3 mins, we think the car has stopped over 3 mins and just start running at the current timestamp
  mutate(stop = ifelse(diff_mins >= 3, TRUE, FALSE)) %>% 
  filter(stop == TRUE) %>% 
  ungroup() %>% 
  arrange(id, Timestamp)
# rearrange useful features
gps2_stop <- gps2[c(7,6,2,3,4,8,5)]

gps2_stop_sf <- st_as_sf(gps2_stop,
                         coords = c("long", "lat"), # combine the lo, la
                         crs = 4326) # 4326 is wgs84 Geographic Coordinate System
```

The "start" in the "gps2_stop_sf" refers to the start time of this car stop period, while the "end" refers to the end time when the car ends parking and start moving.

However we will find the GPS data has multiple strange car stops. One is shown as follows.

```{r}
knitr::kable(gps2_stop_sf[c(5:8),],
             caption = "Anomalies in Car Stop") %>% 
  kableExtra::kable_paper("hover", full_width = F)
```

We can see the end time of the 2nd car stop is the same with the start time of the 3rd car stop. But they should be one car stop originally instead of two car stops. There are a lot of such car stop records. And why this occurred is that there is a single GPS record within an original car stop. This record should not be there and will break the car stop into two after the above code logic.

To fix this, we add one more flag feature to exclude every abnormal GPS records within single original car stop. These records have special timestamp: far away from one preceding timestamp and far away from one following timestamp in records of one car. But for the normal car stop, the timestamp, which is also the end time of this car stop period, refer the time when the car starts moving and should be followed by continuous timestamp. Thus, we can exclude those car stops whose following timestamp is far from itself.

We choose 30 seconds as the cutoff. It means that records will be regarded as abnormal and excluded if the car stop period ends at this timestamp (the car should start moving) but there are no car movement GPS data in 30 seconds.

```{r}
gps2 <- gps
# assign unique index
gps2$idx <- c(1:685169)
gps2_false_stop <- gps2 %>% 
  group_by(id) %>% 
  arrange(Timestamp) %>% 
  mutate(end = Timestamp,
         start = lag(Timestamp, default = first(Timestamp)),
         diff_mins = difftime(end, start, units = "mins"),
         move_time_sec = difftime(lead(Timestamp, n=2), Timestamp, units = "sec")) %>%
  filter(diff_mins >= 3 & move_time_sec > 30) %>% 
  ungroup() %>%
  dplyr::select(start, end, idx, id, lat, long)
false_stop_idx <- gps2_false_stop$idx
# exclude these records
gps2_true <- gps2 %>% 
  filter(!idx %in% false_stop_idx)
# now we can get true stop 
gps2_true_stop <- gps2_true %>% 
  group_by(id) %>% 
  arrange(Timestamp) %>% 
  mutate(end = Timestamp,
         start = lag(Timestamp, default = first(Timestamp)),
         diff_mins = difftime(end, start, units = "mins"),
         move_time_sec = difftime(lead(Timestamp, n=2), Timestamp, units = "sec")) %>%
  filter(diff_mins >= 3 & move_time_sec < 30) %>% 
  ungroup() %>%
  dplyr::select(start, end, diff_mins, diff_mins, id, lat, long, day, idx)
# concatenate GPS into coords
gps2_stop_sf <- st_as_sf(gps2_true_stop,
                         coords = c("long", "lat"), # combine the lo, la
                         crs = 4326) # 4326 is wgs84 Geographic Coordinate
# add unique index 
gps2_stop_sf$idx1 <- c(1:nrow(gps2_stop_sf))
```

For car assignment, most vehicles are assigned one-to-one. Only truck drivers are not assigned cars but are allowed to use available trucks for business purpose.

```{r}
car_assignments <- read_csv("data/car-assignments.csv")
# check car assignment data
knitr::kable(car_assignments,
             caption = "Car Assignment") %>% 
  kableExtra::kable_paper("hover", full_width = F) %>% 
  kableExtra::scroll_box(height = "300px")
```

Let's join the car assignment into the car stop locations, which will add more information while viewing the car line/dot graph on the map

```{r}
gps2_stop_sf <- left_join(gps2_stop_sf, 
                          car_assignments, by = c("id" = "CarID"))
```


For the car movement path, we need to group the GPS data and concatenate them to the simple feature format for the graph. One path is actually one line string with multiple GPS points. 

```{r}
# convert values from numerical to factor data type
gps3 <- gps
gps3$day <- as.factor(gps3$day)
gps3$id <- as_factor(gps3$id)

gps_sf <- st_as_sf(gps3,
                   coords = c("long", "lat"),
                   crs = 4326)
# group car paths
gps_path <- gps_sf %>%
  group_by(id, day) %>%
  summarize(m =mean(Timestamp),
            do_union=FALSE) %>%
  st_cast("LINESTRING")
```


### QGIS

The tourist map provided is not georeferenced. And [QGIS](https://qgis.org/en/site/) can help to georeference an image with the ESRI shapefiles (geospatial vector data) of the city.

The process includes:

1. load JPG tourist map and shp road map
2. create several referencing points between two maps
3. start georeferencing maps and check the correspondence

After the process, we will get a tif file which is a combination of tourist map and georeferenced road map. Then we can plot car movements line with longitude and latitude data on the map.

we need to import the tif file generated by QGIS and display the map.

```{r}
bgmap <- raster("data/Geospatial/MC2-tourist.tif")
```

## Visualization and Insights

```{r}
library(ggplot2)
library(plotly)
library(sf)
library(tmap)
library(clock)
library(ggforce)
```

### Q1: Popular Locations

> Using just the credit and loyalty card data, identify the most popular locations, and when they are popular. What anomalies do you see? What corrections would you recommend to correct these anomalies? 

To identify popularity, we can calculate the card usage frequency and amount in every locations of different days and hours.

Firstly, let's plot the frequency of cards in the 14 days. We need to calculate the card usage frequency in different days, convert into data frame, draw their heatmaps and plot together.

```{r q1f1, layout="l-body-outset", fig.cap = "Daily Frequency of Credit (left) and Loyalty (right) Card Usage", code_folding="Q1-Fig1 Code"}
# calculate the frequency data frame of credit and loyalty card usage
cc_freq_day <- as.data.frame(xtabs(~location+day, data = cc))
loyalty_freq_day <- as.data.frame(xtabs(~location+day, data = loyalty))

# join the two frequency data frame
freq_day_join <- full_join(cc_freq_day,loyalty_freq_day,by= c("location","day"))
names(freq_day_join) <- c("location","day","CC_Freq","Loyalty_Freq")
# transfer from factors to numeric with original values
freq_day_join$day <- as.numeric(levels(freq_day_join$day))[freq_day_join$day]
# plot the heatmap of credit card usage frequency 
p1 <- ggplot(freq_day_join,aes(x=day,y=location))+
  geom_tile(aes(fill=CC_Freq))+
  scale_fill_gradient(low = "#deeff7", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        legend.title=element_blank())
# plot the heatmap of loyalty card usage frequency 
p2 <- ggplot(freq_day_join,aes(x=day,y=location))+
  geom_tile(aes(fill=Loyalty_Freq))+
  scale_fill_gradient(low = "#deeff7", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        legend.title=element_blank())
# convert static graph into interactive
plotly::subplot(ggplotly(p1),
                ggplotly(p2),
                shareY = TRUE)
```

From the card usage frequency (or consumption frequency), we can easily identify that "Katerina’s Café", "Hippokampos" and "Brew've Been Served" are the most popular with almost all squares in deeper color, where the daily consumption frequency is above 10. "Hallowed Grounds" and "Guy's Gyros" are slightly less popular.

Besides, we can find that "Brew've Been Served" and "Hallowed Grounds" are popular every day except weekends (day 11-12, 18-19). The frequency are 0 on weekends, which might because the location is closed on weekends. It's the same to "Hallowed Grounds".

On weekends, "Katerina’s Café" and "Hippokampos" are the most popular while other locations might be closed or less consumption these days.

As for anomalies, we can see there is one white line in the graph for loyalty card, corresponding to "Daily Dealz". This location only have one credit card consumption record on day 13 and no loyalty card record among the two weeks.

The daily frequencies are the same between "Maximum Iron and Steel" and "Kronos Pipe and Irrigation" every day in the two weeks.

To correct these anomalies, we can check the GPS data to make sure who made the only one consumption in "Daily Dealz". If there were no anomalies after checking, we can just delete this single record in the credit card data. And for the situation between "Maximum Iron and Steel" and "Kronos Pipe and Irrigation", it's just coincidence after checking the consumption amount.


Secondly, we can plot the consumption amount instead of frequency. The steps are almost the same.

```{r, eval=FALSE, echo=FALSE}
cc_price_matrix <- tapply(cc$price,cc[,c("location","day")],sum)
cc_price <- reshape2::melt(cc_price_matrix)

loyalty_price_matrix <- tapply(loyalty$price,loyalty[,c("location","day")],sum)
loyalty_price <- reshape2::melt(loyalty_price_matrix)

price_day_join <- full_join(cc_price,loyalty_price,by= c("location","day"))
names(price_day_join) <- c("location","day","Price.","Price")

p1_price <- ggplot(price_day_join,aes(x=day,y=location))+
  geom_tile(aes(fill=Price.))+
  scale_fill_gradient(low = "#deeff7", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        legend.title=element_blank())
p2_price <- ggplot(price_day_join,aes(x=day,y=location))+
  geom_tile(aes(fill=Price))+
  scale_fill_gradient(low = "#deeff7", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        legend.title=element_blank())

plotly::subplot(ggplotly(p1_price),
        ggplotly(p2_price),
        shareY = TRUE)
```

```{r q1f2, fig.cap="Daily Consumption Amount of Credit Card", code_folding="Q1-Fig2 Code"}
cc_price_matrix <- tapply(cc$price,cc[,c("location","day")],sum)
cc_price <- reshape2::melt(cc_price_matrix)
cc_price <- na.omit(cc_price)
p1_price <- ggplot(cc_price,aes(x=day,y=location))+
  geom_tile(aes(fill=value))+
  scale_fill_gradient(low = "#deeff7", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        legend.title=element_blank())

ggplotly(p1_price)
```

```{r q1f3, layout="l-body-outset", fig.height=6, fig.cap="Daily Consumption Amount of Credit Card (box plot)", code_folding="Q1-Fig3 Code"}
plot_ly(cc_price, x = ~value, y = ~location, type = "box",
        boxpoints = "outliers", marker = list(color= 'rgb(255,0,0)')) %>% 
  layout(showlegend = FALSE)
```

The consumption amount differences among locations are much bigger than frequency differences. 

Apparently, "Abila Airport" are the place where has the biggest consumption amount. And these consumption occurred on weekdays only.

Besides, "Stewart and Sons Fabrication", "Nationwide Refinery" and "Abila Airport" also have high consumption amounts. All these locations don't show high frequency values in previous graphs but have very high daily consumption amounts.

And there are many outliers which might be anomalies. For example, "Frydos Autosupply n' More" had a daily cc consumption amount ($10455.22) on day 13, which is several times as much as those in other days. And the "Albert's Fine Clothing" also has a daily consumption outlier on day 17.

What's more, there are many inconsistencies between amounts in the credit card record and loyalty card record. At "Stewart and Sons Fabrication", the daily amounts from day 13 to day 16 don't match in two graphs.

To correct these anomalies, we need to check through the car movement data where the consumption amount outliers exist. It's to see whether there are activities or other gathering to cause the high consumption. As for the inconsistency in amounts, the possible explanations are there might be someone used only one of the two cards or got cashback in the consumption.


Lastly, we change the time unit from days to hours to analyze the popular locations. Only the timestamp of credit card data contains time, so there are no hourly heatmaps for loyalty card usage.


```{r q1f4, layout="l-body-outset", fig.cap="Hourly Consumption Frequency and Amount of Credit Card", code_folding="Q1-Fig4"}
cc_freq_hour <- as.data.frame(xtabs(~location+hour, data = cc))
# convert factor into number
cc_freq_hour$hour <- as.numeric(levels(cc_freq_hour$hour))[cc_freq_hour$hour]

cc_price_hour_matrix <- tapply(cc$price,cc[,c("location","hour")],sum)
cc_price_hour <- reshape2::melt(cc_price_hour_matrix)

cc_hour_join <- full_join(cc_freq_hour, cc_price_hour, by= c("location","hour"))
names(cc_hour_join) <- c("location","hour","Freq","Amount")

p3_freq <- ggplot(cc_hour_join,aes(x=hour,y=location))+
  geom_tile(aes(fill=Freq))+
  scale_fill_gradient(low = "#deeff7", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank())

p3_price <- ggplot(cc_hour_join,aes(x=hour,y=location))+
  geom_tile(aes(fill=Amount))+
  scale_fill_gradient(low = "#deeff7", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank())

plotly::subplot(ggplotly(p3_freq),
        ggplotly(p3_price),
        shareY = TRUE) %>% 
  hide_colorbar()
```

From the left hourly heatmap, we can easily identify the popular period for each locations since there are clear pattern. 

* Breakfast time (07:00 to 08:59): "Brew've Been Served", "Hallowed Grounds"
* Lunch time (12:00 to 1:59): "Katerina’s Café", "Hippokampos", "Abila Zacharo"
* Dinner time (19:00 to 20:59): "Katerina’s Café", "Hippokampos", "Guy's Gyros"

And some anomalies exist in the strange time period. At 3am, there are 5 credit card usages in "Kronos Mart". For "Daily Dealz", the only credit card transaction happened at 6am.

The right heatmap also shows anomalies: the consumption at "Bean There Done That", "Brewed Awakenings", "Jack's Magical Beans" and "Coffee Shack" all happened within 12 o'clock.

```{r}
knitr::kable(cc %>% 
               filter(location %in% c("Bean There Done That","Coffee Shack",
                                      "Jack's Magical Beans","Brewed Awakenings")),
             caption = "Consumption Recorda at Bean There Done That and Coffee Shack ") %>% 
  kableExtra::kable_paper("hover", full_width = F) %>% 
  kableExtra::scroll_box(height = "300px") 
```

We can see that all consumption records at these locations are exactly at 12:00:00. And these locations seems to be coffee shop. There might be something wrong with the machine which records credit card consumption at the two location.

### Q2: GPS Data and Anomalies

> Add the vehicle data to your analysis of the credit and loyalty card data. How does your assessment of the anomalies in question 1 change based on this new data? What discrepancies between vehicle, credit, and loyalty card data do you find?


##### Anomaly 1: High consumption at "Frydos Autosupply n' More" on day 13, Figure \@ref(fig:q1f2)

Firstly, filter out the credit card consumption record at "Frydos Autosupply n' More" on day 13.

```{r}
knitr::kable(cc %>% 
               filter(day == 13 & location == "Frydos Autosupply n' More"),
             caption = "Consumption Records at Frydos Autosupply n' More on day 13") %>% 
  kableExtra::kable_paper("hover", full_width = F)
```

The abnormal consumption is from the cc number 9551. Let's check the consumption records of this cc on day 13.

```{r}
knitr::kable(cc %>% 
               filter(day == 13 & last4ccnum == 9551),
             caption = "Consumption Records of cc 9551 Owner on Day 13") %>% 
  kableExtra::kable_paper("hover", full_width = F)
```

We can see the cc owner make the only only transaction at "Daily Dealz" at early morning (6 o'clock), which is the only one transaction in the two weeks.

Besides, "U-Pump" is a special place because there were only two consumption records in the two weeks, which can be found in Figure \@ref(fig:q1f4) and Figure \@ref(fig:q1f1). Therefore, there should have fewer stop locations near U-Pump in the car GPS data.

We can check the stop locations on day 13 (all dots in Figure \@ref(fig:q2f1)). On this day, there was one point near U-Pump where the stop time is near the consumption time in "U-Pump". The corresponding car id is 24. 

Thus, we think that Minke, the owner of car 24, might use the credit card 9551. Let's draw the moving path of this car to discover more. The stops dots of car 24 are highlight into blue and the its drive path is also plotted with a blue line.

```{r q2f1, fig.cap="Stop Locations and Driving Path of Car 24 on Day 13", code_folding="Q2-Fig1 Code"}
gps2_stop_day13 <- gps2_stop_sf %>% 
  filter(day ==13)

gps2_stop_car24_day13 <- gps2_stop_sf %>% 
  filter(day ==13 & id == 24)

gps_path_car24_day13 <- gps_path %>% 
  filter(day == 13 & id == 24)

map1 <- tm_shape(bgmap) +
  tm_rgb(bgmap, r = 1,g = 2,b = 3,
         alpha = NA,
         saturation = 1,
         interpolate = TRUE,
         max.value = 255) +
  tm_shape(gps_path_car24_day13) +
  tm_lines(col = "blue") +
  tm_shape(gps2_stop_day13) +
  tm_dots() +
  tm_shape(gps2_stop_car24_day13) +
  tm_dots(col = "blue", size = 0.1)
tmap_leaflet(map1)
```

Hovering over blue dots, we can see the stop locations of car 24. On day 13, the car started running at about 7 o'clock from home (the east area in the map) and stopped at "Katerina’s Café" (the south-east area) for half an hour. Then, the car stopped near "Albert's Fine Clothing" at around noon (the north-west area).

After that, the car stopped near "U-Pump" (the center area) from 12:35 to 13:22. The purchase time in "U-Pump", 13:18:00, matches the time period.

From 13:27 to 17:57, the car stopped at the GASTech company (south area), which could be the employee was working. 

After the work, the car stopped near the "Brew've Been Served" (the south-east area) from 18:00 to 19:29. The high consumption occured in this period. The stop location is also close to the "Frydos Autosupply n' More". So the driver might stopped the car and walked to the "Frydos Autosupply n' More" to make the consumption.

There are strange things.

1. The consumption at "Daily Dealz" occurred at 06:04:00, while the car left home at 07:32:01. It's strange that the purchase happedned so early and the location can't be found in other records

2. The consumption at "Hippokampos" occurred at 13:28:00, while the car stopped at the company at 13:27:14. The time gap is about 30 seconds

3. The consumption at "Ouzeri Elian" occurred at 19:30:00, while the car left the "Frydos Autosupply n' More" at 19:29:01. The time gap is just 30 seconds after the car left

We can check the consumption from the combination of credit and loyalty cards data. We use left join to find the corresponding records in the loyalty data.

```{r}
knitr::kable(cc %>% 
               filter(day == 13 & last4ccnum == 9551) %>% 
               left_join(loyalty, by = c("location", "day", "price")),
             caption = "Consumption Records of cc 9551 with Corresponding Loyalty Records on Day 13") %>% 
  kableExtra::kable_paper("hover", full_width = F)

```
We can see that the two consumption records, which have little time gap with the car leaving/stopping, exactly have corresponding loyalty card usage. But the other three consumption records should be less rush but didn't use loyalty card. One possible explaination might be the card stealing. This suspicious activity need to be analyzed further in question 5.

##### Anomaly 2: Mid-night consumption at "Kronos Mart", Figure \@ref(fig:q1f4)

The first step is to find the corresponding records.

```{r}
knitr::kable(cc %>% 
               filter(location == "Kronos Mart"),
             caption = "Consumption at Kronos Mart") %>% 
  kableExtra::kable_paper("hover", full_width = F)

```


The strange consumption records are the second row, the third row and the last 3 rows, which occurred at 3 o'clock.  

The records on day 19 belong to the owners of credit cards 3484, 9551, 8332.

Coincidentally, credit card 9551 also appeared in the Anomaly 1.

Day 19 is one day before the employee missing incident. We can check the car stop points in the recent one week to find the reason or any anomalies.

```{r q2f2, fig.cap="Stop Locations from Day 13 to Day 19", code_folding="Q2-Fig2 Code"}
gps2_stop_days <- gps2_stop_sf %>%
  filter(between(day,13,18))

gps2_stop_day19 <- gps2_stop_sf %>%
  filter(day == 19)

map2 <- tm_shape(bgmap) +
  tm_rgb(bgmap, r = 1,g = 2,b = 3,
         alpha = NA,
         saturation = 1,
         interpolate = TRUE,
         max.value = 255) +
  tm_shape(gps2_stop_days) +
  tm_dots(size = 0.1, alpha = 0.5) +
  tm_shape(gps2_stop_day19) +
  tm_dots(col = "red", size = 0.1, alpha = 0.5)
tmap_leaflet(map2)
```

The "Kronos Mart" is located at the west direction with a red symbol. After zooming the map, we can see there were no car stop location near the mart On day 19 (red dot). And several closer red dot, which located at "Roberts and Sons", were in the afternoon (stop period within 13 to 14 o'clock).

But there are three black dot which are very close to the "Kronos Mart". Furthermore, The three car stop all started at about 13:30 and ends at about 16:00 on day 18. 

The owners of the three cars are listed.

```{r}
gps2_stop_days %>% 
               filter((id == 1 | id == 10 |id == 23) 
                      & day == 18
                      & start > "2014-01-18 13:00:00"
                      & end < "2014-01-18 16:00:00") %>% 
knitr::kable(caption = "The Three Car Stop Near Kronos Mart") %>% 
  kableExtra::kable_paper("hover", full_width=T) %>% 
  kableExtra::scroll_box(width = "100%") 
```

We can't get insights from the car owner information since they belong to different employment type. But they stayed at the same location for similar time period. They are very likely to meet each other and do the same thing. Besides, the consumption at 3 o'clock came from 3 credit cards and this meetup also involved in 3 persons.

Thus, one possible explanation of the consumption at mid-night could be that the three car owners came to discuss some plans on day 18 and met again at 3 o'clock on day 19. 

Another possible explanation direction could be persons just stayed near the mart, so they don't need to drive and walked there to make consumption. Or the three person use other vehicles, not from the company, to reach the mart and make consumption. 

This suspicious activities will be analyzed further in question 5. It might need to check the behaviors of the three car owners in the 14 days.

##### Discrepancies between data


```{r}
cc_num <- length(unique(cc$last4ccnum))
loyalty_num <- length(unique(loyalty$loyaltynum))
ppl_num <- length(car_assignments$LastName)
c(cc_num, loyalty_num, ppl_num)
```

We can find that there are 44 employees, but 55 credit cards and 54 loyalty cards. If we suppose no errors in the card id, one employee has one or more credit cards and one or more loyalty cards.


<!-- ######################################### -->
<!-- check the one car No GPS data visible before the 17th? -->


--------------------------------------------------

Interactive graphs consume a lot of time to load in the page, so this study is divided into two blogs.


**To be continued with [Part 2](https://ygliu.netlify.app/posts/2021-07-31-vast-challenge-2021-mc2-part2/).**

--------------------------------------------------



