---
title: "VAST Challenge 2021 MC2"
description: |
  To visualize & analyze card usage and car movement data with the employee disappearance incident
author:
  - name: LIU Yangguang
    url: https://www.linkedin.com/in/ygliu/
    affiliation: School of Computing and Information Systems, Singapore Management
date: 07-23-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 3
    toc_float: true
    #code_folding: true
categories:
  - R
  - Visualization
  - Plotly
preview: data/MC2-tourist.jpg
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      R.options = list(width = 60))
```


## Background

This study is based on the [Mini-Challenge 2](https://vast-challenge.github.io/2021/MC2.html) of the [VAST Challenge 2021](https://vast-challenge.github.io/2021/). In a fiction scenario, there is a natural gas company named "GASTech" operating in the island country if Kronos. The GASTech didn't do well in environment stewardship. And after an company IPO celebration in January 2014, several employees of GASTech went missing. An environment organization is suspected in the disappearance.

Many of the Abila, Kronos-based employees of GAStech have company cars which are approved for both personal and business use. And the others who don't have company cars can check and use company trucks for business use. The GPS tracking data of these company vehicles is available for the two weeks prior the disappearance.

And the company also provides a loyalty card to employees to give them discounts in the local businesses. And their credit card purchases and loyalty cards usage data are provided. But these data does not have persona information beyond purchases.

### Requirement

Use visual analytics to identify which GASTech employees made which purchases and identify suspicious patterns of behavior. Besides, the study must cope with uncertainties that result from missing, conflicting, and imperfect data to make recommendations for further investigation.

### Questions

1. Using just the credit and loyalty card data, identify the most popular locations, and when they are popular. What anomalies do you see? What corrections would you recommend to correct these anomalies? Please limit your answer to 8 images and 300 words.

2. Add the vehicle data to your analysis of the credit and loyalty card data. How does your assessment of the anomalies in question 1 change based on this new data? What discrepancies between vehicle, credit, and loyalty card data do you find? Please limit your answer to 8 images and 500 words.

3. Can you infer the owners of each credit card and loyalty card? What is your evidence? Where are there uncertainties in your method? Where are there uncertainties in the data? Please limit your answer to 8 images and 500 words.

4. Given the data sources provided, identify potential informal or unofficial relationships among GASTech personnel. Provide evidence for these relationships. Please limit your response to 8 images and 500 words.

5. Do you see evidence of suspicious activity? Identify 1- 10 locations where you believe the suspicious activity is occurring, and why Please limit your response to 10 images and 500 words.

### Literature review

The VAST Challenge 2014 has the same scenario with slightly different dataset and questions. The submission repository can be found [here](http://visualdata.wustl.edu/varepository/VAST%20Challenge%202014/challenges/MC2%20-%20Patterns%20of%20Life%20Analysis/).

Various analytic tools were used among the submissions, like JMP, D3 and custom tools. The heatmap and time histograms were useful to represent the numerical value under the combination of one  categorical variable and one discrete/categorical variable, such as the usage frequency under different locations and days. Besides, movement line graph with the map background  can help to identify and check suspicious activities.

However, almost all graphs were static and readers would find it difficult to explore other parts in graphs which were not specially mentioned by authors. Since the study is displayed on html page, the interactive graphs will be possible. For example, the tooltip function can make every data point to have detailed information without checking the axis or drawing additional graphs. The zoom-in and onclick functions allow readers to check the whole complex graph with too many lines/objects and focus on one part only.  



## Data Preparation

### Data Wrangling

Import packages needed firstly.

```{r}
library(tidyverse)
library(lubridate)
library(raster)
```

The location names contain some special characters, such as "Café", which are not recognized by utf-8 encoding. Thus, special encoding is used in reading data.

```{r}
loyalty <- read_csv("data/loyalty_data.csv", locale=locale(encoding ="windows-1252"))
cc <- read_csv("data/cc_data.csv", locale=locale(encoding ="windows-1252"))
```

The timestamp in the credit card usage date ("cc") contains date and time, while the timestamp in the loyal card usage data ("loyalty") contains only data. Besides, their data type is string, which will be transformed into datetime type.

And we separate day, hour from the datetime feature.

```{r}
loyalty$timestamp <- as.Date(loyalty$timestamp, "%m/%d/%Y")
cc$timestamp <- strptime(cc$timestamp, "%m/%d/%Y %H:%M")

loyalty$day <- mday(loyalty$timestamp)
cc$date <- as.Date(cc$timestamp, "%m/%d/%Y %H:%M")
cc$day <- mday(cc$date)
cc$hour <- hour(cc$timestamp)
```

The timmestamp in the gps data also need to be transformed.

```{r}
gps <- read_csv("data/gps.csv")
gps$Timestamp <- strptime(gps$Timestamp, "%m/%d/%Y %H:%M:%S")
# convert values from numerical to factor data type
gps$day <- as.factor(mday(gps$Timestamp))
gps$id <- as_factor(gps$id)
```


### QGIS

The tourist map provided is not georeferenced. And [QGIS](https://qgis.org/en/site/) can help to georeference an image with the ESRI shapefiles (geospatial vector data) of the city.

The process includes:

1. load JPG tourist map and shp road map
2. create several referencing points between two maps
3. start georeferencing maps and check the correspondence

After the process, we will get a tif file which is a combination of tourist map and georeferenced. road map. Then we can plot car movements line with longitude and latitude data on the map.

## Visualization and Insights

```{r}
library(ggplot2)
library(dplyr)
library(plotly)
```

### Q1: Identify popular locations and anomalies

To identify popularity, we can calculate the card usage frequency and amount in every locations of different days and hours.

Firstly, let's plot the frequency of cards in the 14 days. We need to calculate the card usage frequency in different days, convert into data frame, draw their heatmaps and plot together.

```{r, layout="l-body-outset", code_folding=TRUE}
# calculate the frequency data frame of credit and loyalty card usage
cc_freq_day <- as.data.frame(xtabs(~location+day, data = cc))
loyalty_freq_day <- as.data.frame(xtabs(~location+day, data = loyalty))

# join the two frequency data frame
freq_day_join <- full_join(cc_freq_day,loyalty_freq_day,by= c("location","day"))
names(freq_day_join) <- c("location","day","CC_Freq","Loyalty_Freq")
# transfer from factors to numeric with original values
freq_day_join$day <- as.numeric(levels(freq_day_join$day))[freq_day_join$day]
# plot the heatmap of credit card usage frequency 
p1 <- ggplot(freq_day_join,aes(x=day,y=location))+
  geom_tile(aes(fill=CC_Freq))+
  scale_fill_gradient(low = "#deeff7", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        legend.title=element_blank())
# plot the heatmap of loyalty card usage frequency 
p2 <- ggplot(freq_day_join,aes(x=day,y=location))+
  geom_tile(aes(fill=Loyalty_Freq))+
  scale_fill_gradient(low = "#deeff7", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        legend.title=element_blank())
# convert static graph into interactive
plotly::subplot(ggplotly(p1),
                ggplotly(p2),
                shareY = TRUE) %>% 
  layout(
    title = list(
      text = "Daily Frequency of Credit (left) and Loyalty (right) Card Usage",
      xanchor = "center"),
    font = list(size = 12))
```

From the card usage frequency (or consumption frequency), we can easily identify that "Katerina’s Café", "Hippokampos" and "Brew've Been Served" are the most popular with almost all squares in deeper color, where the daily consumption frequency is above 10. "Hallowed Grounds" and "Guy's Gyros" are slightly less popular.

Besides, we can find that "Brew've Been Served" and "Hallowed Grounds" are popular every day except weekends (day 11-12, 18-19). The frequency are 0 on weekends, which might because the location is closed on weekends. It's the same to "Hallowed Grounds".

On weekends, "Katerina’s Café" and "Hippokampos" are the most popular while other locations might be closed or less consumption these days.

As for anomalies, we can see there is one white line in the graph for loyalty card, corresponding to "Daily Dealz". This location only have one credit card consumption record on day 13 and no loyalty card record among the two weeks.

The daily frequencies are the same between "Maximum Iron and Steel" and "Kronos Pipe and Irrigation" every day in the two weeks.

To correct these anomalies, we can check the GPS data to make sure who made the only one consumption in "Daily Dealz". If there were no anomalies after checking, we can just delete this single record in the credit card data. And for the situation between "Maximum Iron and Steel" and "Kronos Pipe and Irrigation", it's just coincidence after checking the consumption amount.


Secondly, we can plot the consumption amount instead of frequency. The steps are almost the same.

```{r, layout="l-body-outset"}
cc_price_matrix <- tapply(cc$price,cc[,c("location","day")],sum)
cc_price <- reshape2::melt(cc_price_matrix)

loyalty_price_matrix <- tapply(loyalty$price,loyalty[,c("location","day")],sum)
loyalty_price <- reshape2::melt(loyalty_price_matrix)

price_day_join <- full_join(cc_price,loyalty_price,by= c("location","day"))
names(price_day_join) <- c("location","day","Price.","Price")

p1_price <- ggplot(price_day_join,aes(x=day,y=location))+
  geom_tile(aes(fill=Price.))+
  scale_fill_gradient(low = "#deeff7", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        legend.title=element_blank())
p2_price <- ggplot(price_day_join,aes(x=day,y=location))+
  geom_tile(aes(fill=Price))+
  scale_fill_gradient(low = "#deeff7", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        legend.title=element_blank())

plotly::subplot(ggplotly(p1_price),
        ggplotly(p2_price),
        shareY = TRUE) %>% 
  layout(
    title = list(text = "Daily Consumption Amount of Credit and Loyalty Card",
                 xanchor = "center"),
    font = list(size = 12))
```

The consumption amount differences among locations are much bigger than frequency differences. 

Apparently, "Abila Airport" are the place where has the biggest consumption amount. And these consumption occurred on weekdays only.

Besides, "Stewart and Sons Fabrication", "Nationwide Refinery" and "Abila Airport" also have high consumption amounts on weekdays. All these locations don't show high frequency values in previous graphs but have very high daily consumption amounts.

And there are many outliers which might be anomalies. For example, "Frydos Autosupply n' More" had a daily cc consumption amount ($10455.22) in day 13, which is several times as much as those in other days. And the loyalty consumption amount in day 8 at "Nationwide Refinery" is 12554.91.

What's more, there are many inconsistencies between amounts in the credit card record and loyalty card record. At "Stewart and Sons Fabrication", the daily amounts from day 13 to day 16 don't match in two graphs.

To correct these anomalies, we need to check through the car movement data where the consumption amount outliers exist. It's to see whether there are activities or other gathering to cause the high consumption. As for the inconsistency in amounts, the possible explanations are there might be someone used only one of the two cards or got cashback in the consumption.


Lastly, we change the time unit from days to hours to analyze the popular locations. Only the timestamp of credit card data contains time, so there are no hourly heatmaps for loyalty card usage.

```{r, eval=FALSE, echo=FALSE}
p3_freq_ <- ggplot(cc_hour_join,aes(x=hour, y=location, 
                                    text=paste("Amount:", Amount)))+
  geom_tile(aes(fill=Freq))+
  scale_fill_gradient(low = "#deeff7", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank())
ggplotly(p3_freq_)
```


```{r, layout="l-body-outset"}
cc_freq_hour <- as.data.frame(xtabs(~location+hour, data = cc))
# convert factor into number
cc_freq_hour$hour <- as.numeric(levels(cc_freq_hour$hour))[cc_freq_hour$hour]

cc_price_hour_matrix <- tapply(cc$price,cc[,c("location","hour")],sum)
cc_price_hour <- reshape2::melt(cc_price_hour_matrix)

cc_hour_join <- full_join(cc_freq_hour, cc_price_hour, by= c("location","hour"))
names(cc_hour_join) <- c("location","hour","Freq","Amount")

p3_freq <- ggplot(cc_hour_join,aes(x=hour,y=location))+
  geom_tile(aes(fill=Freq))+
  scale_fill_gradient(low = "#deeff7", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank())

p3_price <- ggplot(cc_hour_join,aes(x=hour,y=location))+
  geom_tile(aes(fill=Amount))+
  scale_fill_gradient(low = "#deeff7", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank())

plotly::subplot(ggplotly(p3_freq),
        ggplotly(p3_price),
        shareY = TRUE) %>% 
  hide_colorbar() %>% 
  layout(
    title = list(text = "Hourly Consumption Frequency and Amount of Credit Card",
                 xanchor = "center"),
    font = list(size = 12))
```

From the left hourly heatmap, we can easily identify the popular period for each locations since there are clear pattern. 

* Breakfast time (7am to 8am): "Brew've Been Served", "Hallowed Grounds"
* Lunch time (12pm to 1pm): "Katerina’s Café", "Hippokampos", "Abila Zacharo"
* Dinner time (7pm to 8pm): "Katerina’s Café", "Hippokampos", "Guy's Gyros"

And some anomalies exist in the strange time period. At 3am, there are 5 credit card usages in "Kronos Mart". For "Daily Dealz", the only credit card transaction happened at 6am.

The right heatmaps also show anomalies: A very high consumption amount ($32419.63) happened at 11am of "Stewart and Sons Fabrication".

### Q2: Add vehical data to analyze the anomalies and discrepancies between data

```{r}
packages = c('raster', 'sf', 
             'tmap', 'clock')
for (p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p,character.only = T)
}
```

After loading packages, we need to import the tif file generated by QGIS and display the map.

```{r}
bgmap <- raster("data/Geospatial/MC2-tourist.tif")


tm_shape(bgmap) +
  tm_rgb(bgmap, r = 1,g = 2,b = 3,
         alpha = NA,
         saturation = 1,
         interpolate = TRUE,
         max.value = 255)
```

Then...

```{r, layout="l-body-outset"}
gps_sf <- st_as_sf(gps,
                   coords = c("long", "lat"), # combine the lo, la
                   crs = 4326) # 4326 is wgs84 Geographic Coordinate System

gps_path <- gps_sf %>%
  group_by(id, day) %>%
  summarize(m =mean(Timestamp),
            do_union=FALSE) %>%
  st_cast("LINESTRING")
# filter path by id
gps_path_selected <- gps_path %>%
  filter(id==1)
tmap_mode("view")

tm_shape(bgmap) +
  tm_rgb(bgmap, r = 1,g = 2,b = 3,
         alpha = NA,
         saturation = 1,
         interpolate = TRUE,
         max.value = 255) +
  tm_shape(gps_path_selected) +
  tm_lines()
```


画地图时，参考清华的Q3的流程
需要考虑的包，包括mapview，S8有写

数据差异的话，除了图一图二中的，还有卡的对应关系不一致，车的数据和卡的数据


### Q3: Infer the owners of credit and loyalty card, as well as evidence and uncertainties
S9里的Bipartite：匹配信用卡和会员数据？
美化可以参考-Parallel Sets of https://cheriewpq.shinyapps.io/isss608_group02/
定义所有car stop大于1分钟的作为一次stop，找出所有stop的gps定位。得到停车的开始结束时间+stop的定位
和cc的数据全连接，然后统计匹配航叔的频次？画出Bipartite？

```{r, layout="l-body-outset"}
1
```
```{r, layout="l-body-outset"}
1
```
```{r, layout="l-body-outset"}
1
```

### Q4: Identify potential informal relationships amoung GASTech personnel with evidence
鉴别potential informal or unofficial relationships among GASTech personnel.
```{r, layout="l-body-outset"}
1
```
```{r, layout="l-body-outset"}
1
```

### Q5: Identify locations where might be suspicious activity with evidence
```{r, layout="l-body-outset"}
1
```
```{r, layout="l-body-outset"}
1
```

suspicious activity: Identify 1- 10 locations where you believe the suspicious activity is occurring, and why Please limit your response to 10 images and 500 words.

卡车，用于非商业活动的也算可疑活动
解释可疑活动时，是以某人做了某件事为理由

day*hour的图片
Now, let's divide the units from days into hours:

```{r, layout="l-screen", fig.height=10}
cc_freq_day_hour <- as.data.frame(xtabs(~location++day+hour, data = cc))
cc_freq_day_hour$hour <- as.numeric(levels(cc_freq_day_hour$hour))[cc_freq_day_hour$hour]
p3 <- ggplot(cc_freq_day_hour,aes(x=hour,y=location))+
  geom_tile(aes(fill=Freq),color="white")+
  scale_fill_gradient(low = "#EFF7FB", high = "#0D2330")+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        legend.title=element_blank(),
        plot.title = element_text(hjust=0.5))+
  facet_wrap(~ day, ncol = 7)+
  labs(title = "CC Frequency by hour of the day") 
ggplotly(p3)
```

## Conclusion


