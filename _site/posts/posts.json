[
  {
    "path": "posts/2021-07-23-vast-challenge-2021-mc2/",
    "title": "VAST Challenge 2021 MC2",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "LIU Yangguang",
        "url": "https://www.linkedin.com/in/ygliu/"
      }
    ],
    "date": "2021-07-23",
    "categories": [
      "R",
      "Visualization"
    ],
    "contents": "\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-07-23T23:00:09+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-02-nyc-taxi-tip-prediction/",
    "title": "What Factors Will Influence Taxi Tipping",
    "description": "Perform exploratory data analysis and build ensemble learning models to predict taxi tip amount based on other trip information, geography location and datetime data.",
    "author": [
      {
        "name": "LIU Yangguang",
        "url": "https://www.linkedin.com/in/ygliu/"
      }
    ],
    "date": "2021-07-01",
    "categories": [
      "Python",
      "Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nData Exploration\r\nDataset\r\nData Exploration\r\n\r\nFeature Engineering\r\nModel\r\nModel Building\r\nConclusion\r\n\r\nFuture Work\r\n\r\n /* A floating TOC, but it's not suitable for long TOC*/\r\nhtml {\r\n  scroll-behavior: auto; /* smooth, auto */\r\n}\r\nd-article {\r\n    contain: none;\r\n    overflow-x: hidden;\r\n  }\r\n#TOC {\r\n  position: relative; /* float will make the toc fixed; 'fixed' can make toc float */\r\n  z-index: 50; /* priority when the elements overlap each other  */\r\n  background: white;     /* or#ebebeb; */\r\n  /* optional padding: 10px; border-radius: 5px; */\r\n  }\r\n\r\n/* Hide the ToC when resized to mobile or tablet:  480px, 768px, 900px */\r\n@media screen and (min-height: 55em) and (min-width: 65em) { /* change from 900 , min-width: 80em, min-width: 1000px*/\r\n#TOC {\r\n    position: fixed;\r\n  }\r\n}\r\n\r\nIntroduction\r\nThe taxi fare amount is decided by trip distance, pickup time and so on. It has a built-in calculation formula and is transparent to the public. But the tip amount doesn’t have any fixed rule, which seems to be mostly affected by the qualitative features, such as passengers’ mood and drivers’ service and friendliness.\r\nThe objective of this study is to build predictive models to predict what factors will influence the taxi tip amount. It would help taxi drivers and online ride-hailing platforms to figure out the important factors and add the revenue.\r\nThe study is done using python and jupyter notebook. The code notebook can be found here.\r\nData Exploration\r\nDataset\r\nThe dataset originates from New York City Taxi and Limousine Commission (TLC). It has 1,071,910 samples and 14 features. And each sample/row represents a trip.\r\nFirst, import the data and take a quick peek at how the data looks like.\r\n\r\n\r\n\r\n\r\n\r\n\r\nvendor_id\r\n\r\n\r\npickup_datetime\r\n\r\n\r\ndropoff_datetime\r\n\r\n\r\npickup_longitude\r\n\r\n\r\npickup_latitude\r\n\r\n\r\ndropoff_longitude\r\n\r\n\r\ndropoff_latitude\r\n\r\n\r\nrate_code\r\n\r\n\r\npassenger_count\r\n\r\n\r\ntrip_distance\r\n\r\n\r\npayment_type\r\n\r\n\r\nfare_amount\r\n\r\n\r\ntip_amount\r\n\r\n\r\ntip_paid\r\n\r\n\r\nCMT\r\n\r\n\r\n2013-11-03 18:11:18+00:00\r\n\r\n\r\n2013-11-03 18:18:43+00:00\r\n\r\n\r\n-73.97188\r\n\r\n\r\n40.75623\r\n\r\n\r\n-73.97143\r\n\r\n\r\n40.76418\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.80\r\n\r\n\r\nCSH\r\n\r\n\r\n6.5\r\n\r\n\r\n0.00\r\n\r\n\r\n0\r\n\r\n\r\nCMT\r\n\r\n\r\n2010-06-30 17:30:34+00:00\r\n\r\n\r\n2010-06-30 17:40:44+00:00\r\n\r\n\r\n-73.97785\r\n\r\n\r\n40.73836\r\n\r\n\r\n-73.97707\r\n\r\n\r\n40.75423\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1.50\r\n\r\n\r\nCre\r\n\r\n\r\n6.9\r\n\r\n\r\n1.88\r\n\r\n\r\n1\r\n\r\n\r\nCMT\r\n\r\n\r\n2010-03-03 10:10:25+00:00\r\n\r\n\r\n2010-03-03 10:19:38+00:00\r\n\r\n\r\n-73.97409\r\n\r\n\r\n40.76248\r\n\r\n\r\n-73.99067\r\n\r\n\r\n40.74567\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1.60\r\n\r\n\r\nCre\r\n\r\n\r\n6.9\r\n\r\n\r\n1.00\r\n\r\n\r\n1\r\n\r\n\r\nVTS\r\n\r\n\r\n2012-04-17 11:51:00+00:00\r\n\r\n\r\n2012-04-17 12:07:00+00:00\r\n\r\n\r\n-73.97992\r\n\r\n\r\n40.78104\r\n\r\n\r\n-73.97042\r\n\r\n\r\n40.76561\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1.70\r\n\r\n\r\nCRD\r\n\r\n\r\n9.7\r\n\r\n\r\n1.94\r\n\r\n\r\n1\r\n\r\n\r\nVTS\r\n\r\n\r\n2012-03-30 14:26:00+00:00\r\n\r\n\r\n2012-03-30 14:36:00+00:00\r\n\r\n\r\n-73.97298\r\n\r\n\r\n40.76161\r\n\r\n\r\n-73.95561\r\n\r\n\r\n40.76429\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n1.36\r\n\r\n\r\nCSH\r\n\r\n\r\n6.9\r\n\r\n\r\n0.00\r\n\r\n\r\n0\r\n\r\n\r\n\r\nThere are some features could sound a bit strange. The TLC metadata and taxi fare explanation page give us explanations:\r\nvendor_id: the identifier of the programs that provided the data\r\nrate_code: the identifier of different rates due to the destination, such as trips beyond NYC or airport trips\r\ntip_paid: the boolean value about whether the tip was paid or not\r\nBesides, we can see that the datetime features are in UTC time zone, not the local time zone (US/East).\r\nData Exploration\r\nMissing value\r\n\r\nprint(raw.isnull().sum()/len(raw))\r\n\r\n\r\nMissing value percentage for each feature is:\r\nvendor_id            0.000000\r\npickup_datetime      0.000000\r\ndropoff_datetime     0.000000\r\npickup_longitude     0.000000\r\npickup_latitude      0.000000\r\ndropoff_longitude    0.000000\r\ndropoff_latitude     0.000000\r\nrate_code            0.161596\r\npassenger_count      0.000000\r\ntrip_distance        0.000000\r\npayment_type         0.000000\r\nfare_amount          0.000000\r\ntip_amount           0.000000\r\ntip_paid             0.000000\r\ndtype: float64\r\n0.16159565635174594\r\n\r\nThere is null values only in ‘rate_code’ and the missing value percentage is significant that we can’t just removes these rows with missing values but need to replace them.\r\nCheck Distributions\r\n\r\nraw.info() \r\n\r\n\r\nRangeIndex: 1071910 entries, 0 to 1071909\r\nData columns (total 14 columns):\r\n #   Column             Non-Null Count    Dtype  \r\n---  ------             --------------    -----  \r\n 0   vendor_id          1071910 non-null  object \r\n 1   pickup_datetime    1071910 non-null  object \r\n 2   dropoff_datetime   1071910 non-null  object \r\n 3   pickup_longitude   1071910 non-null  float64\r\n 4   pickup_latitude    1071910 non-null  float64\r\n 5   dropoff_longitude  1071910 non-null  float64\r\n 6   dropoff_latitude   1071910 non-null  float64\r\n 7   rate_code          898694 non-null   float64\r\n 8   passenger_count    1071910 non-null  int64  \r\n 9   trip_distance      1071910 non-null  float64\r\n 10  payment_type       1071910 non-null  object \r\n 11  fare_amount        1071910 non-null  float64\r\n 12  tip_amount         1071910 non-null  float64\r\n 13  tip_paid           1071910 non-null  int64  \r\ndtypes: float64(8), int64(2), object(4)\r\nmemory usage: 114.5+ MB\r\n\r\nThere are 4 categorical features and 10 numerical features. But we know that ‘rate_code’ is an identifier thus a categorical feature.\r\nCategorical Features\r\n\r\ncategory_fea = ['vendor_id', 'payment_type', 'rate_code']\r\n\r\nfig, ax = plt.subplots(1, 3, figsize=(100, 30))\r\nfor i in range(3):\r\n    se = raw[category_fea[i]].value_counts(dropna=False)\r\n    x = [str(x) for x in se.index]\r\n    y = list(se.values)\r\n    p = ax[i].barh(x,y)\r\n    ax[i].tick_params(labelsize=70)\r\n    ax[i].set_title(category_fea[i], fontsize=100)\r\n    ax[i].xaxis.set_ticks(np.linspace(0,800000,3))\r\n\r\n\r\nWe can find that there are many long tail value in the ‘payment_type’ and ‘rate_code’. Only one or two values dominate the feature. They need to be bucketized later to avoid extremely unbalanced distributions.\r\nAnd for the ‘payment_type’. It has some different values with the same meaning. Like ‘Cre’ and ‘CRE’ both refer to the credit card payment. ‘CAS’ and ‘Cas’ both refer to the cash payment.\r\nBesides, we don’t plot the two datetime features here because they are the combination of date and time and have hundreds of thousands unique values. The distribution will be displayed after the data transformation.\r\nNumerical Features\r\n\r\nnumerical_fea = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', \r\n                  'passenger_count', 'trip_distance', 'fare_amount', 'tip_amount']\r\nf = pd.melt(raw, value_vars=numerical_fea) # melt the col\r\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=4, sharex=False, sharey=False)\r\ng = g.map(sns.histplot, \"value\")\r\n\r\n The four latitude and longitude features approximately follow normal distributions, but we can’t put them into the model without processing. The locations, which can be regard as binning of geography features, are more important than values.\r\n\r\nraw[['trip_distance','fare_amount','tip_amount']].describe(\r\n    percentiles=[.25, .50, .75, .99, .999])\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ntrip_distance\r\n\r\n\r\nfare_amount\r\n\r\n\r\ntip_amount\r\n\r\n\r\ncount\r\n\r\n\r\n1.071910e+06\r\n\r\n\r\n1.071910e+06\r\n\r\n\r\n1.071910e+06\r\n\r\n\r\nmean\r\n\r\n\r\n1.812045e+00\r\n\r\n\r\n8.456763e+00\r\n\r\n\r\n7.843356e-01\r\n\r\n\r\nstd\r\n\r\n\r\n1.221569e+00\r\n\r\n\r\n4.017411e+00\r\n\r\n\r\n1.190242e+00\r\n\r\n\r\nmin\r\n\r\n\r\n1.000000e-02\r\n\r\n\r\n2.500000e+00\r\n\r\n\r\n0.000000e+00\r\n\r\n\r\n25%\r\n\r\n\r\n9.500000e-01\r\n\r\n\r\n5.700000e+00\r\n\r\n\r\n0.000000e+00\r\n\r\n\r\n50%\r\n\r\n\r\n1.500000e+00\r\n\r\n\r\n7.500000e+00\r\n\r\n\r\n0.000000e+00\r\n\r\n\r\n75%\r\n\r\n\r\n2.300000e+00\r\n\r\n\r\n1.010000e+01\r\n\r\n\r\n1.500000e+00\r\n\r\n\r\n99%\r\n\r\n\r\n6.090000e+00\r\n\r\n\r\n2.150000e+01\r\n\r\n\r\n4.300000e+00\r\n\r\n\r\n99.90%\r\n\r\n\r\n8.700000e+00\r\n\r\n\r\n3.150000e+01\r\n\r\n\r\n7.000000e+00\r\n\r\n\r\nmax\r\n\r\n\r\n1.680000e+01\r\n\r\n\r\n2.000000e+02\r\n\r\n\r\n1.444000e+02\r\n\r\n\r\nAnd ‘trip_distance’, ‘fare_amount’ and ‘tip_amount’ all have extremely outliers. Take ‘tip_amount’ as an example, 99.9% values are less than 7, but the max value reaches 144.4. It’s the same situation with ‘trip_amount’ and ‘fare_amount’. Because this study will focus on how much the tip amount would be in the general situation instead of the extreme situations, these outliers will be excluded later.\r\nFeature Engineering\r\nMissing value\r\nWe can take a look at the rows with missing value by the handy function below.\r\n\r\nimport pandas_profiling\r\n\r\ndf_null = raw.iloc[list(raw['rate_code'].isnull()),:] # filter out the rows with missing value\r\nreport = pandas_profiling.ProfileReport(df_null)\r\nreport.to_widgets()\r\n\r\nThe result can be found here. But there is no significant findings about the feature distribution of these rows with missing value.\r\nThus, we can only replace the missing value from its onwn distribution. And because one value take the majority, we will group the missing value and other values into one group.\r\n\r\nraw['rate_code_group'] = raw['rate_code'].apply(lambda x:'1' if x == 1.0 else 'Others')\r\n# check the new distribution\r\nprint(raw['rate_code_group'].value_counts())\r\n\r\n\r\n1         896903\r\nOthers    175007\r\nName: rate_code_group, dtype: int64\r\n\r\nOutliers\r\n\r\n# draw the box plot\r\nfig,ax2 = plt.subplots(3,1,figsize=(15,10))\r\nsns.boxplot(x=\"trip_distance\", data=raw, ax=ax2[0])\r\nsns.boxplot(x=\"fare_amount\",  data=raw, ax=ax2[1])\r\nsns.boxplot(x=\"tip_amount\", data=raw, ax=ax2[2])\r\n# clean outliers\r\nidx = (raw['tip_amount'] < 30) & (raw['fare_amount'] < 60) & (raw['trip_distance'] < 12.5)\r\nraw_clean = raw.loc[idx]\r\n\r\n\r\nIn total, We excluded 83 extreme outlier samples out of the 1 million raw data. It will help improve the model performance as well as the robustness.\r\nDatetime Features\r\nThe datetime features are transformed from string to datetime and switch to the local timezone in the New York. Thus we can get the real datetime data. We can find that the date cover from 2009 to 2015.\r\n\r\n# convert the string into datetime\r\nraw_clean['pickup_datetime'] = raw_clean['pickup_datetime'].apply(\r\n    lambda dt: datetime.strptime(dt, \"%Y-%m-%d %H:%M:%S%z\"))\r\n# convert time zone\r\nus_east = timezone('US/Eastern')\r\nraw_clean['pickup_datetime'] = raw_clean['pickup_datetime'].apply(\r\n    lambda dt: dt.astimezone(us_east))\r\n\r\n# do the same transformation for dropoff datetime\r\nraw_clean['dropoff_datetime'] = raw_clean['dropoff_datetime'].apply(\r\n    lambda dt: datetime.strptime(dt, \"%Y-%m-%d %H:%M:%S%z\"))\r\nraw_clean['dropoff_datetime'] = raw_clean['dropoff_datetime'].apply(\r\n    lambda dt: dt.astimezone(us_east))\r\n# take a look at the date distribution\r\ntime_date = raw_clean['pickup_datetime'].apply(lambda dt: dt.date)\r\ntime_date_count = time_date.value_counts().reset_index().sort_values(by='index', ascending=True)\r\nsns.lineplot(x='index', y='pickup_datetime', data=time_date_count)\r\nplt.gcf().autofmt_xdate() # format the date x-axis\r\n\r\n\r\nInstead of using these datetime features, we need to extract some new features from them to put in models:\r\npickup_hour: The number of hour when the pickup happened (24 hour-hour clock)\r\npickup_weekday: The weekday name when the pickup happened\r\npickup_month: The month name when the pickup happened\r\npickup_year: The year number when the pickup happened\r\nmin_period: The minutes between pickup time and dropoff time\r\n\r\n# get the hour number of pickup time\r\nraw_clean['pickup_hour'] = raw_clean['pickup_datetime'].apply(lambda dt: int(dt.hour))\r\n# get the weekday\r\nraw_clean['pickup_weekday'] = raw_clean['pickup_datetime'].apply(lambda dt: dt.strftime(\"%A\"))\r\n# get the month of the year\r\nraw_clean['pickup_month'] = raw_clean['pickup_datetime'].apply(lambda dt: dt.strftime(\"%b\"))\r\n# get the year\r\nraw_clean['pickup_year'] = raw_clean['pickup_datetime'].apply(lambda dt: str(dt.year))\r\n# get the time period (minitues) between pickup and dropoff\r\ntimeperiod = raw_clean['dropoff_datetime'] - raw_clean['pickup_datetime']\r\nraw_clean['min_period'] = timeperiod.apply(lambda x: float(x.seconds/60))\r\n\r\nGeography Features\r\nWe can draw the scatter plot between longitude and latitude.\r\n\r\nfig, ax4 = plt.subplots(figsize=(100, 100))\r\n\r\nax4.scatter('pickup_longitude', 'pickup_latitude', data=raw_clean, s=5, color='blue', alpha=0.5)\r\nax4.scatter('dropoff_longitude', 'dropoff_latitude', data=raw_clean, s=5, color='red', alpha=0.5)\r\n\r\n\r\nThe high density area is Manhattan actually. The points seems to draw many grids, which might corresponds to the building. It would be a good idea to name the geography location into the corresponding neighborhood. But that need much more computation resource and dataset. Here we take a simple but still useful method: bin the longitude and latitude.\r\nBinning\r\n\r\n# bucketize longgitude & lagitude by percentile\r\nraw_clean['pickup_lo_cut'] = pd.cut(raw_clean['pickup_longitude'], 20)\r\nraw_clean['pickup_la_cut'] = pd.cut(raw_clean['pickup_latitude'], 20)\r\nraw_clean['dropoff_lo_cut'] = pd.cut(raw_clean['dropoff_longitude'], 20)\r\nraw_clean['dropoff_la_cut'] = pd.cut(raw_clean['dropoff_latitude'], 20)\r\n\r\nWe bucketize the four geography feature into 20 bins.\r\nAnd the ‘payment_type’ also need to be bucketing since it has long tail values.\r\n\r\n# combine 'Cre' and 'CRE', 'Cas' and 'CAS'\r\nraw_clean['payment_type_group'] = raw_clean['payment_type'].apply(lambda x: str.upper(x))\r\n# combine the minor into one group\r\nraw_clean['payment_type_group'] = raw_clean['payment_type_group'] = raw_clean['payment_type_group'].apply(\r\n    lambda x: 'Other' if x not in ['CSH','CRD','CAS','CRE'] else x)\r\n\r\nBesides, the ‘pickup_hour’ should be divided into groups as well. Here we bin 24 hours into 4 time period: ‘Midnight’(0-6), ‘Morning’(6-12), ‘Afternoon’(12-18), ‘Evening’(18-23)\r\n\r\n# group the pickup hour\r\nbins = [0, 6, 12, 18, 23]\r\nraw_clean['pickup_hour_group'] = pd.cut(raw_clean['pickup_hour'], bins, include_lowest = True,\r\n                                        labels=['Midnight', 'Morning', 'Afternoon', 'Evening'])\r\n\r\nAfter these data transformation, we can exclude some old features and keep the new features extracted from them. Then we can get a new dataset with 16 features.\r\n\r\ndrop_list = ['pickup_datetime', 'dropoff_datetime', 'pickup_longitude',\r\n             'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', \r\n             'rate_code', 'payment_type', 'pickup_hour', 'tip_paid']\r\n\r\nraw_clean = raw_clean.drop(drop_list, axis=1)\r\n\r\nThe last step before modelling is to perform one-hot encoding for categorical variables:\r\n\r\ndummy_list = ['vendor_id', 'rate_code_group', 'pickup_hour_group', 'pickup_weekday',\r\n              'pickup_month', 'pickup_year', 'pickup_lo_cut', 'pickup_la_cut', \r\n              'dropoff_lo_cut', 'dropoff_la_cut', 'payment_type_group']\r\n\r\nraw_clean_dummy = pd.get_dummies(raw_clean, columns=dummy_list, drop_first=True)\r\n\r\nModel\r\nModel Building\r\nThe linear regression would not perform well since the distribution of the independent features and target feature. Thus, We will use a decision tree as our base model. And ensemble learning models based on the decision tree will be built to make comparison. The objective metrics is root-mean-square error (RMSE).\r\nFirst, the cleansed data are split into train data and test data.\r\n\r\nx_train, x_test, y_train, y_test = train_test_split(\r\n    raw_clean_dummy.drop(['tip_amount'], axis=1), raw_clean_dummy['tip_amount'], test_size=0.2)\r\n\r\nThe base model is decision tree regressor. The calculated RMSE on the test data is 0.6747.\r\n\r\nbase = DecisionTreeRegressor(criterion='mse', min_samples_leaf = 3, random_state = 2021)\r\n\r\ndt = base.fit(x_train, y_train)\r\nbase_pred = dt.predict(x_test)\r\nbase_rmse = metrics.mean_squared_error(base_pred, y_test, squared = False)\r\n\r\nThe ensemble learning models we use are Random Forest, AdaBoost and Gradient Boosting. To get the optimal hyperparameter combination, we will split the train data into the train and the validation. And the KFold cross validation is used in hyperparameter tuning because it can ensure more “trustable” estimation.\r\nThe hyperparameters chosen to test are mostly related to the model complexity. The complexity model might not always get better performance. So we want to test and get the optimal model with different complexity.\r\nRandom Forest\r\n\r\n# 5 fold cross validation\r\ncv_fold = KFold(n_splits=5, random_state=2021, shuffle=True)\r\n\r\n# RandomForestRegressor\r\nmodel_rf = ensemble.RandomForestRegressor()\r\n# the hyperparameter range to tune\r\npara_rf = {\"n_estimators\": [40, 60, 80, 100, 120],\r\n        \"max_depth\": range(4,11,1),\r\n        \"min_samples_split\": range(3,11,1)} \r\n\r\n# hyperparameter tunning\r\nrf_randomd_search = RandomizedSearchCV(estimator=model_rf,\r\n                                       cv=cv_fold,\r\n                                       n_iter = 5, \r\n                                       param_distributions= para_rf,\r\n                                       scoring='neg_root_mean_squared_error',\r\n                                       n_jobs = -1) # use all processors\r\n# fit the train data\r\nrf = rf_randomd_search.fit(x_train, y_train)\r\n# Train the model again with the calculated best parameters\r\nrf_best_para = rf.best_params_\r\nrf_best = ensemble.RandomForestRegressor(n_estimators = rf_best_para[\"n_estimators\"],\r\n                                         max_depth = rf_best_para[\"max_depth\"],\r\n                                         min_samples_split = rf_best_para['min_samples_split'],\r\n                                         random_state = 2021)\r\nrf_best.fit(x_train, y_train)\r\n\r\n# predict on the test data\r\nrf_pred = rf_best.predict(x_test)\r\n# calculate the RMSE\r\nrf_rmse = metrics.mean_squared_error(y_test, rf_pred, squared = False)\r\nprint('The RMSE of RandomForestRegressor on the test data is', rf_rmse)\r\n\r\n# Print the top 5 important feature\r\ncol = x_train.columns\r\nrf_fea_importance = rf_best.feature_importances_\r\nrf_index = np.argsort(rf_fea_importance)[::-1]\r\nfor i in range(5):\r\n    print(f'No.{i+1}, {col[rf_index[i]]}')\r\n\r\nAdaBoost\r\n\r\nmodel_ada = ensemble.AdaBoostRegressor()\r\n# the hyperparameter range to be test\r\npara_ada = {\"n_estimators\": [40, 60, 80, 100, 120],\r\n            \"learning_rate\": [0.1, 0.5, 1]}\r\n\r\n# hyperparameter tunning\r\nada_randomd_search = RandomizedSearchCV(estimator=model_ada,\r\n                                        cv=cv_fold,\r\n                                        n_iter = 5, \r\n                                        param_distributions= para_ada,\r\n                                        scoring='neg_root_mean_squared_error',\r\n                                        n_jobs = -1)\r\n# fit the train data\r\nada = ada_randomd_search.fit(x_train, y_train)\r\n# Train the model again with the calculated best parameters\r\nada_best_para = ada.best_params_\r\nada_best = ensemble.AdaBoostRegressor(n_estimators = ada_best_para[\"n_estimators\"],\r\n                                      learning_rate = ada_best_para[\"learning_rate\"],\r\n                                      random_state=2021)\r\nada_best.fit(x_train, y_train)\r\n\r\n# predict on the test data\r\nada_pred = ada_best.predict(x_test)\r\n# calculate the RMSE\r\nada_rmse = metrics.mean_squared_error(y_test, ada_pred, squared = False)\r\nprint('The RMSE of AdaBoostRegressor on the test data is', ada_rmse)\r\n\r\n# Print the top 5 important feature\r\nada_fea_importance = ada_best.feature_importances_\r\nada_index = np.argsort(ada_fea_importance)[::-1]\r\nfor i in range(5):\r\n    print(f'No.{i+1}, {col[ada_index[i]]}')\r\n\r\nGradientBoosting\r\n\r\nmodel_gbr = ensemble.GradientBoostingRegressor()\r\n# the hyperparameter range to be test\r\npare_gbr = {\"loss\": ['ls', 'lad', 'huber'],\r\n            \"n_estimators\": [50, 100, 150],\r\n            \"learning_rate\": [0.1, 0.5, 1],\r\n            \"min_samples_split\": [4,8]}\r\n\r\n# hyperparameter tunning\r\ngbr_randomd_search = RandomizedSearchCV(estimator=model_gbr,\r\n                                        cv=cv_fold,\r\n                                        n_iter = 5,\r\n                                        param_distributions= pare_gbr,\r\n                                        scoring='neg_root_mean_squared_error',\r\n                                        n_jobs = -1)\r\n# fit the train data\r\ngbr = gbr_randomd_search.fit(x_train, y_train)\r\n\r\n# Train the model again with the calculated best parameters\r\ngbr_best_para = gbr.best_params_\r\ngbr_best = ensemble.GradientBoostingRegressor(loss = gbr_best_para[\"loss\"],\r\n                                              n_estimators = gbr_best_para[\"n_estimators\"],\r\n                                              learning_rate = gbr_best_para[\"learning_rate\"],\r\n                                              min_samples_split = gbr_best_para[\"min_samples_split\"],\r\n                                              random_state = 2021)\r\ngbr_best.fit(x_train, y_train)\r\n\r\n# predict on the test data\r\ngbr_pred = gbr_best.predict(x_test)\r\n# calculate the RMSE\r\ngbr_rmse = metrics.mean_squared_error(y_test, gbr_pred, squared = False)\r\nprint('The RMSE of GradientBoostingRegressor on the test data is', gbr_rmse)\r\n\r\n# Print the top 5 important feature\r\ngbr_rea_importance = gbr_best.feature_importances_\r\ngbr_index = np.argsort(gbr_rea_importance)[::-1]\r\nfor i in range(5):\r\n    print(f'No.{i+1}, {col[gbr_index[i]]}')\r\n\r\nConclusion\r\nThe model performance on the test data is below:\r\nClassifier\r\nDecidion Tree\r\nRandomForest\r\nAdaBoost\r\nGradientBoosting\r\nRMSE\r\n0.6747\r\n0.5317\r\n0.6182\r\n0.5322\r\nWe can find that the overall three ensemble learning models reduce the RMSE compared with the base model. And Random Forest is the best with the 0.5317 on RMSE, while the gradient boosting performs nearly well.\r\nFurthermore, we print the five most important features in the three models:\r\nClassifier\r\nRandomForest\r\nAdaBoost\r\nGradientBoosting\r\nFeature 1\r\npayment_type_group_CRD\r\npayment_type_group_CRD\r\npayment_type_group_CRD\r\nFeature 2\r\nfare_amount\r\nfare_amount\r\nfare_amount\r\nFeature 3\r\npayment_type_group_CRE\r\ntrip_distance\r\npayment_type_group_CRE\r\nFeature 4\r\nmin_period\r\npayment_type_group_CRE\r\nmin_period\r\nFeature 5\r\nvendor_id_VTS\r\npayment_type_group_CSH\r\npayment_type_group_Other\r\nSurprisingly, the top two important features are the same in the three models. Whether the taxi trip is paid by ‘CRD’ is the most important. And the ‘fare_amount’ is the second important. It implies that the tip amount would be bigger if the fare amount is bigger and the payment type is ‘CRD’.\r\nBesides, different payment type has different influence on the tip amount but many but all payment types are very important in models.\r\nAnd the ‘min_period’ and ‘trip_distance’ are important as well. They can be explained as the same with ‘fare_amount’. A longer trip in distance or driving time will usually have a larger fare amount. And such trip will get a larger tip amount.\r\nFuture Work\r\nThis study is based on my assignment in the machine learning course. There are many parts to improve when time is enough:\r\nThe geography features were simply binning. But a better processing approach can be combine the latitude and longitude to get the real neighborhoods and we would find which specific area are more likely to get a higher tip amount\r\nThe date features can consider the holiday date, which intuitively is an important feature in tipping . It need to manually collect the holiday data from 2009 to 2015 because there is complete datasets with one-click\r\nThe business analysis part is brief. It’s partially because the dataset provided in class has minor difference with the dataset in New York City TLC. And the metadata cannot correspond completely and some values of features don’t have definition\r\nThe EDA can explore more about the relationships between trip features and the tipping\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-02-nyc-taxi-tip-prediction/bg.png",
    "last_modified": "2021-07-02T22:33:49+08:00",
    "input_file": {},
    "preview_width": 2000,
    "preview_height": 1585
  },
  {
    "path": "posts/2021-06-14-dataviz-makeover-2/",
    "title": "DataViz Makeover 2",
    "description": "In this post, we present a trade data use case using Tableau. We will critique the original visualisation from clarity and aesthetic and propose an alternative design.",
    "author": [
      {
        "name": "LIU Yangguang",
        "url": "https://www.linkedin.com/in/ygliu/"
      }
    ],
    "date": "2021-06-20",
    "categories": [
      "Tableau",
      "Makeover"
    ],
    "contents": "\r\n\r\nContents\r\n1.0 Critique of Visualization\r\n2.0 Alternative Design\r\n3.0 Proposed Visualization on Tableau\r\n4.0 Step-by-step Preparation\r\n5.0 Derived Insights\r\n\r\n1.0 Critique of Visualization\r\nThe original visualization is shown as below.\r\n\r\nSource: Department of Statistics, Singapore\r\n1.1 Clarity\r\nImproper centre point labels(white circle): For countries with small trade surplus(deficit), it’s difficult to identify which parts the centre point of the bubble falls in. For example, readers can’t tell whether the merchandise trade with Japan and EU are trade surplus or deficit. Their centre point seems to be on the dividing line of green and blue area.\r\nInconsistent conclusion: Readers can’t draw the conclusion in the blue square from the charts shown. The charts shows the trade performance with ten major partners in 2020. But the conclusion describe the trade performance for Mainland China and US from 2009 and 2006.\r\nOverlap of bubbles: The bubbles of different countries overlap each other. The green circle for Malaysia and the brown circle for Korea are almost hidden by other countries. It would be much better if the bubble are partially tranparent.\r\nIncorrect axis label position: Although readers can tell from color that the vertical axis is blue and corresponds to the blue circle symbol(Imports) at the bottom and that the horizontal axis is green and corresponds to the green symbol at the left, it’s against the condition that the vertical axis label is at the left of the origin and the horizontal label is at the bottom of the origin. Exchanging the current label position can make it less misleading for readers.\r\n1.2 Aesthetic\r\nImproper symbols of top net exporter/importer: The circle with star and arrows sign add unnecessary complexity to the whole chart. Although they looks cute, but it requires reader to read legends for them while they only appear once. In most cases, readers can tell the top net traders approximately from the centre point of bubbles.\r\nInappropriate color for countries: The color of bubbles can help to distinguish between countries. But the color temperature can be misleading for the importance. The red circle for Taiwan and yellow circle for Japan have high color temperature and are eye-catching while they are not the predominate in numerical variables. On the other hand, the green circle for Malaysia and the cyan circle for Mainland China seem to be less important while they have the highest trade amount.\r\nOverlap between country labels and bubbles: Some labels for countries are not placed properly. For example, the label for US hide the centre point and a large part of the bubble for Taiwan. Instead, it could be placed at the right of the US bubble, where is less crowded. The overlap between country labels and bubbles are distracting and prevent readers on focus on one countries.\r\nNo tick marks: For continuous variables, the axis should have tick marks.\r\n2.0 Alternative Design\r\nThe sketch of the proposed design is as follow:\r\n\r\n2.1 Clarity\r\nIt’s easier to identify net importers and net exporters by bubble color\r\nThe 10 year trade performance is in the toolkit and add more information about each countries\r\nNo overlapping between bubbles and labels\r\nInteractive select every year to show the yearly trade performance\r\n2.2 Aesthetic\r\nLess notes and marks to reduce the complexity of charts\r\nCorrect axis labels and consistent color\r\n3.0 Proposed Visualization on Tableau\r\n\r\nThe link to the interactive chart on Tableau Public Server can be found here.\r\n4.0 Step-by-step Preparation\r\nNo.\r\nStep\r\nAction\r\n1\r\nImport the data into Tableau and click “Cleaned with Data Interpreter” to get the cleaned table\r\n\r\n2\r\nDrag and drop T1 sheet to the canvas area\r\n\r\n3\r\nSelect all the date variables, right click and choose the pivot to make date variables into one\r\n\r\n4\r\nChoose the T2 sheet and use the same approach to get the pivoted table\r\n\r\n5\r\nRight click on the date variable and change the data type from string to date in the two tables\r\n\r\n6\r\nRight click on the numerical variable and create a calculated field in the two tables to convert the value into the original number\r\n\r\n7\r\nRight click on the market name variable and choose to split to get the cleaned market name in the two tables\r\n\r\n8\r\nHide the useless variables: raw numerical variable, raw market name variable and the split unit variable in the two tables\r\n\r\n9\r\nRename the date and market name variable into meaningful names\r\n\r\n10\r\nClick the line between tables in the canvas and edit the relationship in order to join them by the same date and the same market name\r\n\r\n11\r\nIn a new sheet, drag and drop Imports to the ‘Rows’, Exports to the ‘Columns’\r\n\r\n12\r\nDrag and drop Date2 into the Filter panel and select Year. The interested scope are only from 2011 to 2020 and choose these years only\r\n\r\n13\r\nDrag and drop Market to the Detail under the Marks panel\r\n\r\n14\r\nDrag and drop Market to the Filters panel and choose the 10 major markets by entering search text\r\n\r\n15\r\nDrag and drop Date into the Filter panel and select ‘Year’. Right click on the new variables under Filters panel to choose ‘Show Filters’\r\n\r\n16\r\nEdit the filter to only show relevant values and not to show all value. Change the style to ‘Single Value (list)’\r\n\r\n17\r\nCreate a new calculated field to get the sum of Imports and Exports. Then drag it into the ‘Size’ under Marks panel\r\n\r\n18\r\nChange the default style to ‘Circle’ and make the size bigger under the Marks panel\r\n\r\n19\r\nCreate a new calculated field to get the trade surplus number for each year (Exports - Imports). Then drag and drop it to the ‘Color’ under the mark panel\r\n\r\n20\r\nEdit ‘Colour’ under Marks panel to make the bubble partially transparent with white border and have 10 stepped color to show the rank of trade surplus\r\n\r\n21\r\nCreate a new sheet and drag Date to ‘Columns’, Imports and Exports to ‘Rows’\r\n\r\n22\r\nRight click on the ‘Exports’ and select ‘Dual Axis’\r\n\r\n23\r\nEdit color of two lines to correspond to the color map in the first chart\r\n\r\n24\r\nEdit x axis into 2 digit format and the y axis to cancel ‘Include Zero’ and title content.\r\n\r\n25\r\nRight click on the dual axis and select ‘Synchronize Axis’ and cancel ‘Show Header’\r\n\r\n26\r\nDrag the chart border to make it small enough to put in the ‘Tooltip’\r\n\r\n27\r\nGo back to the first chart and drag Date into ‘Tooltip’ under Marks panel. Then edit it into ‘AVG(YEAR(Date))’\r\n\r\n28\r\nCreate a new calculated field to get the top net exporter and importer\r\n\r\n29\r\nDouble click ‘Tooltip’ and edit the content into concise sentence, make numbers’ fonts bigger and insert the line graph\r\n\r\n30\r\nEdit the format of x and y axis to change number into custom format. Then change the x header color into blue and the y header color into red\r\n\r\n31\r\nDrag Top Net Trader, Market, Total Trade into ‘Label’ and\r\n\r\n32\r\nRight click ‘Top Net Trade’ label and edit ‘Compute using’ into ‘Market’. Edit ‘Total Trade’ to change the format of numbers\r\n\r\n33\r\nClick ‘Label’ under Marks panel and change its font to ‘Match Mark Color’\r\n\r\n34\r\nEdit the two axis to make the range into fixed value\r\n\r\n35\r\nCreate a calculate field named ‘Reference Line’. Drag it into ‘Rows’, click ‘Dual Axis’ and make the two axis synchronized and ‘not show header’\r\n\r\n36\r\nChange the reference line into ‘Line’ format and right click on the line to choose to show trend line\r\n\r\n37\r\nDrag the chart into a new dashboard and edit the title, add a foot note to show notes and data source\r\n\r\n5.0 Derived Insights\r\nTop Trade Market: China and Malaysia are the two top trading partners over past 10 years. 2013 was a turning point: Malaysia was the top trading partner for Singapore in 2013 and before, but after 2013, China took Malaysia’s place to be the Singapore’s top trading partner. On the other hand, merchandise trade between two countries and Singapore declined sharply in 2016 but turned growth in the following year.\r\n\r\n\r\n\r\nHong Kong: Hong Kong is a typical trade net exporter. Between 2011 and 2020, the trade export amount to Hong Kong are about SGD 6 billion and exports amount are under SGD 1 billion. Imports and exports amounts are very stable over last ten years.\r\n\r\nTaiwan: Over the past decade, both imports and exports have grown a lot overall. Same with China and Malaysia, there was a decline in imports in 2016. But After 2016, imports and exports both grew steadily. And imports grew faster than exports. This led to a increasing amount of the trade surplus.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-14-dataviz-makeover-2/final.png",
    "last_modified": "2021-06-29T11:49:34+08:00",
    "input_file": {},
    "preview_width": 1896,
    "preview_height": 1024
  },
  {
    "path": "posts/2021-05-28-dataviz-makeover-1/",
    "title": "DataViz Makeover 1",
    "description": "A makeover of the data visualisation on merchandise trade in Singapore.",
    "author": [
      {
        "name": "LIU Yangguang",
        "url": "https://www.linkedin.com/in/ygliu/"
      }
    ],
    "date": "2021-05-29",
    "categories": [
      "Tableau",
      "Makeover"
    ],
    "contents": "\r\n\r\nContents\r\n1.0 Critique of Visualization\r\n2.0 Alternative Design\r\n3.0 Proposed Visualization\r\n4.0 Step-by-step Preparation\r\n5.0 Derived Insights\r\n\r\n1.0 Critique of Visualization\r\nThe original visualization is shown as below.\r\n\r\n1.1 Clarity\r\nThe dual axis in each chart have different ranges. It would make readers difficult to compare imports and exports within one country.\r\nThe range of y axis in the six charts are not consistent. Readers need extra effort to compare the trade amount among different countries. They have to look at the y-axis scales separately in the six charts to get the numbers of imports and exports.\r\nThe six charts have different widths. The different widths will disturb readers about how to interpret the change of trade amounts over time. If the width of the same x-axis is bigger in the graph, the data fluctuation will seem to be smaller.\r\nThe order of the six charts is inaccurate. The title is “Top Six Trading Countries”, which makes the reader think that the six charts are ordered by trade amount of each country.\r\n1.2 Aesthetic\r\nThe x-axis scale is confusing. It is not consistent with the axis title. The title is shown as “Month”, but the tick marks are year. Besides, the x-axis of the “Japan” is not the same as the x-axis of other countries.\r\nFont size and color are confusing. The gray color of the head’s font is consistent with the black font of other labels. Besides, the font of the country labels is too large, even larger than the title.\r\nThe coordinates are repeated six times. They take up so much space in the picture that the area of charts are very small.\r\n2.0 Alternative Design\r\nThe proposed design is shown as bellow.\r\n\r\n2.1 Clarity\r\nThe dual axis use the same scale in the six charts. It makes it easier to compare imports and exports amounts or trade amount among countries.\r\nThe six charts have the same height and width.\r\nThe six charts are reorder by the total merchandise trade amount in the period.\r\n2.2 Aesthetic\r\nThe x axis titles are changed to the “Date”, consistent with the tick marks. All charts have the same axis.\r\nThe color of all text is unified. The country fonts have been changed to smaller font sizes to highlight the captions of the images\r\nThe unnecessary axis title are hidden.\r\n3.0 Proposed Visualization\r\n\r\nThe link to the original dashboard on Tableau Public Server can be found here.\r\n4.0 Step-by-step Preparation\r\nThe original data is from Department of Statistics in Singapore, available under the sub-section of Merchandise Trade by Region/Market.\r\nNo.\r\nStep\r\nAction\r\n1\r\nImport the data into Tableau and click “Cleaned with Data Interpreter” to get the cleaned table\r\n\r\n2\r\nDrag and drop T1 sheet to the canvas area\r\n\r\n3\r\nSelect all the date variables, right click and choose the pivot to make date variables into one\r\n\r\n4\r\nChoose the T2 sheet and use the same approach to get the pivoted table\r\n\r\n5\r\nRight click on the “variables” and select “split” to get the cleaned region name, then rename it into “Region”\r\n\r\n6\r\nHide “Variable” and “Variable - split 2” and rename the variables into meaningful name\r\n\r\n7\r\nChange data type of the “Date” variable from string to date in the two table\r\n\r\n8\r\nEdit the connection between two tables to add one rule\r\n\r\n9\r\nAdd the “Date” variable into the Filters panel and select only 2019 and 2020\r\n\r\n10\r\nAdd the “Region” variable into the Filters panel and select the six region in the original designed visualization\r\n\r\n11\r\nAdd “Region” and “Date”(month) to the Columns; Add “Exports” to the Rows; Change the marks from Automatic to Area\r\n\r\n12\r\nDrag the “Imports” to the left of the charts to get the dual axis\r\n\r\n13\r\nEdit the dual y axis into the same range: 0 to 8,000,000\r\n\r\n14\r\nEdit the x axis into the fixed range\r\n\r\n15\r\nRight click on the “Region” and choose sort to manual sort the six regions by the total merchandise trade amount from 2019 to 2020\r\n\r\n16\r\nIt’s too crowded for the six graph in one columns to read the data trend: Rearrange them into two graph. Duplicate the worksheet and use filter to select the corresponding three countries in the two worksheet\r\n\r\n17\r\nDrag and drop the two worksheet to the dashboard\r\n\r\n18\r\nHide the title and the field labels in the second sheet; Hide the x header in the first sheet; Hide the “Imports” header in the two sheets\r\n\r\n19\r\nRename the title, headers and labels for better clarity\r\n\r\n20\r\nChange the font and tick color to be consistent and publish the dashboard to Tableau Public\r\n\r\n5.0 Derived Insights\r\nSingapore has consistently exported much more to Hong Kong than it has imported from China. The trade surplus is the biggest among the six countries. And the merchandise imports from Hong Kong is about the top 2, but exports to Hong Kong are the lowest among the six countries.\r\nSingapore has a significant trade deficit with Taiwan. The imports are about twice as large as exports in 2019 and 2020. Moreover, monthly imports and monthly exports fluctuate very closely: they increase and decrease at the same month in 2019 and 2020.\r\nThe merchandise trade in Singapore is balanced - not overly relies on any single country. There is no big difference in imports, exports or total amounts of the top three countries.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-28-dataviz-makeover-1/cover.png",
    "last_modified": "2021-06-21T16:32:25+08:00",
    "input_file": {},
    "preview_width": 1000,
    "preview_height": 800
  }
]
